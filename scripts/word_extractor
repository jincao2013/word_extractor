#!/usr/bin/env python

import sys
import os
import nltk
# from bs4 import BeautifulSoup
from nltk.corpus import PlaintextCorpusReader
from nltk.stem import WordNetLemmatizer
from nltk.stem.snowball import SnowballStemmer
from corpora.lib import *

__date__ = "Oct. 27, 2023"

if __name__ == "__main__":

    # parameters
    num_coca_exclude = 3000  # this will exclude the top 3000 coca words
    corpora_path = os.path.join(os.path.split(__file__)[0], r'../corpora')
    input_fname = sys.argv[1]
    save_fname = input_fname + '.html'

    # load corpus
    corpus_coca20000 = PlaintextCorpusReader(corpora_path, 'coca-20000-lemma.txt')
    corpus_ielts_zhenjing = PlaintextCorpusReader(corpora_path, 'ielts_zhenjing.txt')
    corpus_longman3000 = PlaintextCorpusReader(corpora_path, 'Longman Communication 3000.txt')

    # check nltk_data
    check_nltk_data()

    # load article
    with open(input_fname, 'r') as f:
        article = f.read()

    coca_words, longman_words_hard, ielts_words_hard = extract_word_list(article, corpus_coca20000, corpus_longman3000, corpus_ielts_zhenjing, num_coca_exclude)
    write_html(article, coca_words, longman_words_hard, ielts_words_hard, fname=save_fname)

